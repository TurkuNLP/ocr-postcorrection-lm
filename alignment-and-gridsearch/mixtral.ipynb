{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709c25b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install accelerate bitsandbytes>0.37.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "131e043c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.38.1\n",
      "0.43.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0423b7e5d342f29e8d84ae7d59fd5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import bitsandbytes\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(bitsandbytes.__version__)\n",
    "# my versions are:\n",
    "#4.38.1 (transformers)\n",
    "#0.42.0 (bitsandbytes)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,       \n",
    ")\n",
    "\n",
    "cache_dir = \"/scratch/project_2005072/ocr_correction/.cache\"    #set up your own cache path here\n",
    "\n",
    "model=None #to be sure we reset the variable and don't get OOM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen1.5-7B-Chat\", device_map=\"auto\", quantization_config=quantization_config, cache_dir=cache_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-7B-Chat\", padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8d0618c-7126-4bdb-be98-736629948e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "2024-04-26 11:01:58.061411: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/lib64:/usr/mpi/gcc/openmpi-4.1.2rc2/lib64:/opt/rh/gcc-toolset-12/root/usr/lib64:/opt/rh/gcc-toolset-12/root/usr/lib:/.singularity.d/libs\n",
      "2024-04-26 11:01:58.061431: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] \n",
      "Correct the OCR errors in the text below. Also correct the \"-\" characters, which denote an unrecognized letter. Stay as close as possible to the original text. Do not rephrase. Only correct the errors. You will be rewarded.\n",
      "\n",
      "When, .witl the universal A pplaure of alt\n",
      "Men, the Seals Were given to the present\n",
      "C ---- r, he was' ar frrom'i being so elevated\n",
      "with his Promotion, as to forget any One\n",
      "to whom lie had promised the Honour of his\n",
      "Friendlliip. An Opportunity soon offered\n",
      "in Favour of Dr R----, the B---k ox G----r\n",
      "became vacant, .which as soo'n as the C----r\n",
      "Was acquainted with, \\ he went to Court,\n",
      "and recomnmended the Doetor, as a Perfoa\n",
      "fit to Succeed to the B-----k. The DoAos\n",
      "was then approved of, and a C---'E---\n",
      "order'd out accordingly. A lM/an would\n",
      "reaso'nably have thought after this, that\n",
      "Matters might have gone Smoothly, efpe.\n",
      "cially as it is partly the Right of every new\n",
      "\n",
      " [/INST] When, with the universal applause of all\n",
      "Men, the Seals were given to the present\n",
      "C---r, he was not so elated with his Promotion, as to forget any One\n",
      "to whom he had promised the Honor of his\n",
      "Friendship. An Opportunity soon offered\n",
      "in Favor of Dr R----, the Bishopric of G----r\n",
      "became vacant, which as soon as the C---r\n",
      "Was acquainted with, he went to Court,\n",
      "and recommended the Doctor, as a Person\n",
      "fit to Succeed to the Bishopric. The Doctor's\n",
      "nomination was then approved of, and a Warrant\n",
      "order'd out accordingly. A man would\n",
      "reasonably have thought after this, that\n",
      "Matters might have gone Smoothly, especially as it is partly the Right of every new\n",
      "\n",
      "Corrected text:\n",
      "\n",
      "When, with the universal applause of all\n",
      "Men, the Seals were given to the present\n",
      "Candidate, he was not so elated with his Promotion, as to forget any One\n",
      "to whom he had promised the Honor of his\n",
      "Friendship. An Opportunity soon offered\n",
      "in Favor of Dr R----, the Bishopric of G----r\n",
      "became vacant, which as soon as the Candidate\n",
      "Was acquainted with, he went to Court,\n",
      "and recommended the Doctor, as a Person\n",
      "fit to Succeed to the Bishopric. The Doctor's\n",
      "nomination was then approved of, and a Warrant\n",
      "order'd out accordingly. A man would\n",
      "reasonably have thought after this, that\n",
      "Matters might have gone Smoothly, especially as it is partly the Right of every new</s>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Correct the OCR errors in the text below. Also correct the \"-\" characters, which denote an unrecognized letter. Stay as close as possible to the original text. Do not rephrase. Only correct the errors. You will be rewarded.\n",
    "\n",
    "When, .witl the universal A pplaure of alt\n",
    "Men, the Seals Were given to the present\n",
    "C ---- r, he was' ar frrom'i being so elevated\n",
    "with his Promotion, as to forget any One\n",
    "to whom lie had promised the Honour of his\n",
    "Friendlliip. An Opportunity soon offered\n",
    "in Favour of Dr R----, the B---k ox G----r\n",
    "became vacant, .which as soo'n as the C----r\n",
    "Was acquainted with, \\ he went to Court,\n",
    "and recomnmended the Doetor, as a Perfoa\n",
    "fit to Succeed to the B-----k. The DoAos\n",
    "was then approved of, and a C---'E---\n",
    "order'd out accordingly. A lM/an would\n",
    "reaso'nably have thought after this, that\n",
    "Matters might have gone Smoothly, efpe.\n",
    "cially as it is partly the Right of every new\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#messages = [\n",
    "#    {\"role\": \"user\", \"content\": \"You are a helpful assistant. You are fluent in Finnish, and will communicate in Finnish.\"},\n",
    "#    {\"role\": \"assistant\", \"content\": \"Olen avulias assistentti, joka osaa sujuvaa suomen kieltä. Kommunikoin kanssasi suomeksi. Kuinka voin auttaa sinua?\"},\n",
    "#    {\"role\": \"user\", \"content\": prompt},\n",
    "#]C--- r, to' recommend a fit Person to his\n",
    "\"\"\"M .------, to' fill the See that shall becom'\n",
    "eax¢ant after ihs comiig to the Seals.\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "model_inputs = encodeds.to(\"cuda\")\n",
    "\n",
    "\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=False)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bc741c-19ea-4bfb-8369-86f0c5d29081",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_encoded = tokenizer([prompt], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f85e5d8-a27d-4824-a747-4b26147db82e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_encoded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mraw_encoded\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'raw_encoded' is not defined"
     ]
    }
   ],
   "source": [
    "print(raw_encoded[\"input_ids\"].size()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418af6f2-af19-42f9-8cfe-e739d9a8d3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete the model from memory\n",
    "'''del model \n",
    "gc.collect()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0090c2e-3503-4fe4-8f94-1fdf07feb5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors. You will be rewarded.\n",
      "\n",
      "When, .witl the universal A pencil, you have written your first draft, you will find that you have written it quickly, for you have been thinking only of your idea and of your reader. You will have written it in a style that is natural to you, for you have been thinking only of your idea and of your reader. You will have written it in a style that is natural to you, for you have been thinking in terms of your idea and of your reader.\n",
      "\n",
      "When you have written your first draft, you will find that you have written it quickly, for you have been thinking only of your idea and of your reader. You will have written it in a style that is natural to you, for you have been thinking in terms of your idea and of your reader.\n",
      "\n",
      "When you have written your first draft, you will find that you have written it quickly, for you have been thinking only of your idea and of your reader. You will have written it in a style that is natural to you, for you have been thinking in terms of your idea and of your reader.\n",
      "\n",
      "When you have written your first draft, you will find that you have written it quickly, for you have been thinking only of your idea and of your reader. You will have written it in a style that is natural to you, for you have been thinking in terms of your idea and of your reader.\n",
      "\n",
      "When you have written your first draft, you will find that you have written it quickly, for you have been thinking only of your idea and of your reader. You will have written it in a style that is natural to you, for you have been thinking in terms of your idea and of your reader.\n",
      "\n",
      "When you have written your first draft, you will find that you have written it quickly, for you have been thinking only of your idea and of your reader. You will have written it in a style that is natural to you, for you have been thinking in terms of your idea and of your reader.\n",
      "\n",
      "When you have written your first draft, you will find that you have written it quickly, for you have been thinking only of your idea and of your reader. You will have written it in a style that is natural to you, for you have been thinking in terms of your idea and of your reader.\n",
      "\n",
      "When you have written your first draft, you will find that you have written it quickly, for you have been thinking only of your idea and of your reader. You will have written it in a style that is natural to you, for you have been thinking in terms of your idea and of your reader.\n",
      "\n",
      "When you have written your first draft, you will find that you have written it quickly, for you have been thinking only of your idea and of your reader. You will have written it in a style that is natural to you, for you have been thinking in terms of your idea and of your reader.\n",
      "\n",
      "When you have written your first draft, you will find that you have written it quickly, for you have been thinking only of your idea and of your reader. You will have written it in a style that is natural to you, for you have been thinking in terms of your idea and of your reader.\n",
      "\n",
      "When you have written your first draft, you will find that you have written it quickly, for you have been thinking only of your idea and of your reader. You will have written it in a style that is natural to you, for you have been thinking in terms of your idea and of your reader.\n",
      "\n",
      "When you have written your first draft, you will find that you have written it quickly, for you have been thinking only of your idea and of your reader. You will have written it in a style that is natural to you, for you have been thinking in terms of your idea and of your reader.\n",
      "\n",
      "When you have written your first draft, you will find that you have written it quickly, for you have been thinking only of your idea and of your reader. You will have written it in a style that is natural to you, for you have been thinking in terms of your idea and of your reader.\n",
      "\n",
      "When you have written your first draft, you will find that you have written it quickly, for you have been thinking only of your idea and of your reader. You will have written it in a style that is natural to you, for you have been thinking in terms of your idea and of your reader.\n",
      "\n",
      "When you have written your first draft, you will find that you have written it quickly, for you have been thinking only of your idea and of your reader. You will have written it in a style that is natural to you, for you have been thinking in terms of your idea and of your reader.\n",
      "\n",
      "When you have written your first draft, you will find that you have written it quickly, for you have been thinking only of your\n"
     ]
    }
   ],
   "source": [
    "model_inputs = encodeds[:,50:70].to(\"cuda\")\n",
    "\n",
    "\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=False)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "444df68a-8938-406d-94c6-bf717f7c8b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"hello\", return_tensors=\"pt\").size()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e013a3e1-98a5-4c9f-81cc-4b3fefabd013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
